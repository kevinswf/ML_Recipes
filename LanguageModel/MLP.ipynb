{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import random"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read data and split by each line\n",
    "words = open('names.txt', 'r').read().splitlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32033"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['emma',\n",
       " 'olivia',\n",
       " 'ava',\n",
       " 'isabella',\n",
       " 'sophia',\n",
       " 'charlotte',\n",
       " 'mia',\n",
       " 'amelia',\n",
       " 'harper',\n",
       " 'evelyn']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words[:10]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Vocab of char to int mapping and int to char"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{1: 'a', 2: 'b', 3: 'c', 4: 'd', 5: 'e', 6: 'f', 7: 'g', 8: 'h', 9: 'i', 10: 'j', 11: 'k', 12: 'l', 13: 'm', 14: 'n', 15: 'o', 16: 'p', 17: 'q', 18: 'r', 19: 's', 20: 't', 21: 'u', 22: 'v', 23: 'w', 24: 'x', 25: 'y', 26: 'z', 0: '.'}\n"
     ]
    }
   ],
   "source": [
    "# get all the unique chars in the dataset\n",
    "chars = sorted(list(set(''.join(words))))  # set removes duplicate chars and keep unique\n",
    "# map each char to a int\n",
    "ctoi = {c: i+1 for i, c in enumerate(chars)}\n",
    "# add a special char, for constructing bigrams of the first and last character in a word\n",
    "ctoi['.'] = 0\n",
    "\n",
    "# int to char\n",
    "itoc = {i: c for c, i in ctoi.items()}\n",
    "\n",
    "print(itoc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([182625, 3]) torch.Size([182625])\n",
      "torch.Size([22655, 3]) torch.Size([22655])\n",
      "torch.Size([22866, 3]) torch.Size([22866])\n"
     ]
    }
   ],
   "source": [
    "# build dataset\n",
    "input_size = 3  # context length, i.e. how many input chars to take to predict the next one, 3 according to Bengio et al.\n",
    "def build_dataset(words):\n",
    "    X, Y = [], []\n",
    "\n",
    "    for word in words:\n",
    "        input = [0] * input_size  # pad the full input with '.' to predict the first char\n",
    "        \n",
    "        # iterate through the chars in the word, to add to X and Y, add the last special '.' char to predict end word\n",
    "        for char in word + '.':\n",
    "            # add the current input, i.e. the 3 chars, to X\n",
    "            X.append(input)\n",
    "\n",
    "            # add the next char as Y\n",
    "            ix = ctoi[char]\n",
    "            Y.append(ix)\n",
    "\n",
    "            # shift the input right by one char\n",
    "            input = input[1:] + [ix]\n",
    "\n",
    "    # convert to tensor\n",
    "    X = torch.tensor(X)\n",
    "    Y = torch.tensor(Y)\n",
    "    print(X.shape, Y.shape)\n",
    "\n",
    "    return X, Y\n",
    "\n",
    "random.seed(42)\n",
    "\n",
    "# shuffle the whole dataset\n",
    "random.shuffle(words)\n",
    "\n",
    "split1_idx = int(0.8 * len(words))\n",
    "split2_idx = int(0.9 * len(words))\n",
    "\n",
    "# create train, val, test set\n",
    "X_train, Y_train = build_dataset(words[:split1_idx])        # training set is for training the model parameters, i.e. weights and bias etc\n",
    "X_val, Y_val = build_dataset(words[split1_idx:split2_idx])  # validation set is for finding the hyperparameters, i.e. lr etc, and also validate no overfitting\n",
    "X_test, Y_test = build_dataset(words[split2_idx:])          # test set for evaluating the model on unseen data\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator = torch.Generator().manual_seed(42)\n",
    "num_emb = 10\n",
    "\n",
    "# embed the 27 chars into 10D embedding, note Bengio et al. is 17000 words to 30\n",
    "lookup = torch.randn((27, num_emb), generator=generator)  # the look up table from 27 chars to 10D"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hidden layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_hidden = 200\n",
    "\n",
    "# random init weights, note no need bias at this layer because batch norm layer follows, and just use batch norm bias\n",
    "W1 = torch.randn((num_emb * input_size, num_hidden), generator=generator)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Output layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_output = 27  # 27 chars\n",
    "\n",
    "# init weights and bias\n",
    "W2 = torch.randn((num_hidden, num_output), generator=generator)\n",
    "b2 = torch.randn(num_output, generator=generator)\n",
    "\n",
    "# batch normalization parameters, init gain to 1 and bias to 0, so that batch norm inits with unit gaussian (and gets scaled and shifted as trained)\n",
    "bn_gain = torch.ones((1, num_hidden))\n",
    "bn_bias = torch.zeros((1, num_hidden))\n",
    "\n",
    "# mean and std to calculate batch norm, for the total train set, i.e. batch = full training set, unit gaussian so mean = 0, std = 1\n",
    "bn_mean_total = torch.zeros((1, num_hidden))\n",
    "bn_std_total = torch.ones((1, num_hidden))\n",
    "\n",
    "parameters = [lookup, W1, W2, b2, bn_gain, bn_bias]\n",
    "for p in parameters:\n",
    "    p.requires_grad = True  # need to manually set grad to true, because PyTorch init these to false on leaf tensors"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      0/ 200000: 22.0277\n",
      "  10000/ 200000: 2.8827\n",
      "  20000/ 200000: 2.1919\n",
      "  30000/ 200000: 2.3755\n",
      "  40000/ 200000: 2.4962\n",
      "  50000/ 200000: 2.1934\n",
      "  60000/ 200000: 2.7461\n",
      "  70000/ 200000: 3.1319\n",
      "  80000/ 200000: 2.4625\n",
      "  90000/ 200000: 2.1207\n",
      " 100000/ 200000: 2.2976\n",
      " 110000/ 200000: 1.7111\n",
      " 120000/ 200000: 1.9515\n",
      " 130000/ 200000: 2.4266\n",
      " 140000/ 200000: 2.2742\n",
      " 150000/ 200000: 1.8444\n",
      " 160000/ 200000: 2.2548\n",
      " 170000/ 200000: 2.2890\n",
      " 180000/ 200000: 2.5596\n",
      " 190000/ 200000: 2.7489\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor(2.3187, grad_fn=<NllLossBackward0>)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MAX_EPOCH = 200000\n",
    "log_epoch = 10000\n",
    "batch_size = 32\n",
    "# reasoning of minibatch:\n",
    "# it's better to run more training steps with approximate gradient (minibatch)\n",
    "# vs running fewer steps with exact gradient (full dataset)\n",
    "\n",
    "epochs = []\n",
    "losses = []\n",
    "\n",
    "for epoch in range(MAX_EPOCH):\n",
    "    # create minibatch training data for this pass\n",
    "    batch_idx = torch.randint(0, X_train.shape[0], (batch_size,))\n",
    "\n",
    "    # embed inputs\n",
    "    emb = lookup[X_train[batch_idx]]\n",
    "\n",
    "    # hidden layer neuron output, i.e. WX + b, where X is the output of embedding (reshaping embedding for input)\n",
    "    hidden_out = emb.view(-1, num_emb * input_size) @ W1\n",
    "\n",
    "    # batch normalization (across the batch samples), so that ouput is normally distributed (before activation function), since this prevents vanishing and exploding gradients\n",
    "    bn_mean_epoch = hidden_out.mean(dim=0, keepdim=True)\n",
    "    bn_std_epoch = hidden_out.std(dim=0, keepdim=True)\n",
    "    hidden_norm = bn_gain * (hidden_out - bn_mean_epoch) / bn_std_epoch + bn_bias\n",
    "\n",
    "    # calculate the running bn mean and std as the data trains\n",
    "    with torch.no_grad():\n",
    "        bn_mean_total = 0.999 * bn_mean_total + 0.001 * bn_mean_epoch\n",
    "        bn_std_total = 0.999 * bn_std_total + 0.001 * bn_std_epoch\n",
    "\n",
    "    # tanh activation\n",
    "    hidden = torch.tanh(hidden_norm)\n",
    "    # output layer neuron output logits\n",
    "    logits = hidden @ W2 + b2\n",
    "\n",
    "    # cross entropy loss, which is just softmax(logits) to get probability, then average negative log likelihood of the probability\n",
    "    loss = F.cross_entropy(logits, Y_train[batch_idx])\n",
    "\n",
    "    # reset gradients\n",
    "    for p in parameters:\n",
    "        p.grad = None\n",
    "    \n",
    "    loss.backward()  # backprop to calculate gradients\n",
    "\n",
    "    # lr decay\n",
    "    lr = 0.1 if epoch < 0.5 * MAX_EPOCH else 0.01\n",
    "\n",
    "    # update params\n",
    "    for p in parameters:\n",
    "        p.data += -lr * p.grad\n",
    "\n",
    "    # stats\n",
    "    epochs.append(epoch)\n",
    "    losses.append(loss.item())\n",
    "    if epoch % log_epoch == 0:\n",
    "        print(f'{epoch:7d}/{MAX_EPOCH:7d}: {loss.item():.4f}')\n",
    "\n",
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x1e367da2940>]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plot the loss over epochs\n",
    "plt.plot(epochs, losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(2.2089)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# evaluate on validation set\n",
    "with torch.no_grad():\n",
    "    emb = lookup[X_val]\n",
    "    hidden_out = emb.view(-1, num_emb * input_size) @ W1\n",
    "    hidden_norm = bn_gain * (hidden_out - bn_mean_total) / bn_std_total + bn_bias\n",
    "    hidden = torch.tanh(hidden_norm)\n",
    "    logits = hidden @ W2 + b2\n",
    "    val_loss = F.cross_entropy(logits, Y_val)\n",
    "\n",
    "val_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "zila.\n",
      "marsony.\n",
      "ditan.\n",
      "dair.\n",
      "aletil.\n",
      "nriydah.\n",
      "jami.\n",
      "coabredde.\n",
      "myli.\n",
      "khirenis.\n"
     ]
    }
   ],
   "source": [
    "# create words from the trained model\n",
    "\n",
    "# sample 10 words\n",
    "for i in range(10):\n",
    "    created_word = []\n",
    "\n",
    "    # init input with all '.' to predict the first char\n",
    "    input = [0] * input_size\n",
    "\n",
    "    # sample and create one word\n",
    "    while True:\n",
    "        # embed input\n",
    "        emb = lookup[torch.tensor([input])]\n",
    "\n",
    "        # forward pass\n",
    "        hidden_out = emb.view(1, -1) @ W1\n",
    "        hidden_norm = bn_gain * (hidden_out - bn_mean_total) / bn_std_total + bn_bias\n",
    "        hidden = torch.tanh(hidden_norm)\n",
    "        logits = hidden @ W2 + b2\n",
    "\n",
    "        # softmax logits to get probabilities\n",
    "        proba = F.softmax(logits, dim=1)\n",
    "\n",
    "        # sample a char from the probability\n",
    "        idx = torch.multinomial(proba, num_samples=1, generator=generator).item()\n",
    "\n",
    "        # append the char\n",
    "        created_word.append(itoc[idx])\n",
    "\n",
    "        # shift the input by one char\n",
    "        input = input[1:] + [idx]\n",
    "\n",
    "        if idx == 0:\n",
    "            break\n",
    "\n",
    "    # print the sampled word\n",
    "    print(''.join(created_word))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Torchify"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Linear layer, simplified version of torch.nn.linear\n",
    "class Linear:\n",
    "    \n",
    "    def __init__(self, fan_in, fan_out, bias=True):\n",
    "        # kaiming init weights\n",
    "        self.weight = torch.randn((fan_in, fan_out), generator=generator) / fan_in**0.5\n",
    "        # init bias to 0 (no need bias if a batch norm layer follows)\n",
    "        self.bias = torch.zeros(fan_out) if bias else None\n",
    "\n",
    "    # linear layer is wx+b\n",
    "    def __call__(self, x):\n",
    "        self.out = x @ self.weight\n",
    "        if self.bias is not None:\n",
    "            self.out += self.bias\n",
    "        return self.out\n",
    "\n",
    "    # function to return all the trainable parameters of this layer\n",
    "    def parameters(self):\n",
    "        return [self.weight] + ([] if self.bias is None else [self.bias])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# batch norm layer, simplified version of torch.nn.BatchNorm1D\n",
    "class BatchNorm1D:\n",
    "\n",
    "    def __init__(self, dim, eps=1e-5, momentum=0.1, training=True):\n",
    "        self.eps = eps\n",
    "        self.momentum = momentum\n",
    "\n",
    "        # batch norm runs differently depending if training, or eval/testing, so need this\n",
    "        self.training = training\n",
    "\n",
    "        # batch_norm scale and shift, trained by backprop\n",
    "        self.gamma = torch.ones(dim)    # it's like weight\n",
    "        self.beta = torch.zeros(dim)    # it's like bias\n",
    "\n",
    "        # running batch norm mean and std of the full training dataset, calculated as training runs\n",
    "        self.running_mean = torch.zeros(dim)    # unit gaussian, mean = 0\n",
    "        self.running_var = torch.ones(dim)      # unit gaussian, std = 1\n",
    "\n",
    "    def __call__(self, x):\n",
    "        if self.training:\n",
    "            # if training, use the mean and std of the training batch\n",
    "            bn_mean = x.mean(dim=0, keepdim=True)\n",
    "            bn_var = x.var(dim=0, keepdim=True)\n",
    "        else:\n",
    "            # else use the mean and std of the full training dataset, calulcated during the training\n",
    "            bn_mean = self.running_mean\n",
    "            bn_var = self.running_var\n",
    "\n",
    "        x_norm = (x - bn_mean) / torch.sqrt(bn_var + self.eps)  # normalize to unit variance, from batch norm paper\n",
    "        self.out = self.gamma * x_norm + self.beta  # scale and shift\n",
    "\n",
    "        # track the running mean and std of the full training dataset, during training only\n",
    "        if self.training:\n",
    "            # no need gradient\n",
    "            with torch.no_grad():\n",
    "                self.running_mean = (1 - self.momentum) * self.running_mean + self.momentum * bn_mean\n",
    "                self.running_var = (1 - self.momentum) * self.running_var + self.momentum * bn_var\n",
    "\n",
    "        return self.out\n",
    "\n",
    "     # function to return all the trainable parameters of this layer\n",
    "    def parameters(self):\n",
    "        return [self.gamma, self.beta]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tanh activation\n",
    "class Tanh:\n",
    "\n",
    "    def __call__(self, x):\n",
    "        self.out = torch.tanh(x)\n",
    "        return self.out\n",
    "    \n",
    "    def parameters(self):\n",
    "        return []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "46924\n"
     ]
    }
   ],
   "source": [
    "# create a MLP model, with more hidden layers\n",
    "num_emb = 10\n",
    "num_hidden = 100\n",
    "vocab_size = 27  # 27 chars\n",
    "\n",
    "lookup = torch.randn((vocab_size, num_emb), generator=generator)  # the look up table from 27 chars to 10D\n",
    "layers = [\n",
    "    Linear(num_emb * input_size, num_hidden), Tanh(),\n",
    "    Linear(num_hidden, num_hidden, bias=False), BatchNorm1D(num_hidden), Tanh(),\n",
    "    Linear(num_hidden, num_hidden, bias=False), BatchNorm1D(num_hidden), Tanh(),\n",
    "    Linear(num_hidden, num_hidden, bias=False), BatchNorm1D(num_hidden), Tanh(),\n",
    "    Linear(num_hidden, num_hidden, bias=False), BatchNorm1D(num_hidden), Tanh(),\n",
    "    Linear(num_hidden, vocab_size, bias=False), BatchNorm1D(vocab_size)\n",
    "]\n",
    "\n",
    "# get all trainable parameters\n",
    "parameters = [lookup] + [p for layer in layers for p in layer.parameters()]\n",
    "print(sum(p.nelement() for p in parameters))\n",
    "for p in parameters:\n",
    "    p.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      0/ 200000: 3.6111\n",
      "  10000/ 200000: 2.2454\n",
      "  20000/ 200000: 2.5648\n",
      "  30000/ 200000: 2.2262\n",
      "  40000/ 200000: 2.1577\n",
      "  50000/ 200000: 2.4081\n",
      "  60000/ 200000: 2.0779\n",
      "  70000/ 200000: 1.9741\n",
      "  80000/ 200000: 2.1684\n",
      "  90000/ 200000: 2.2641\n",
      " 100000/ 200000: 1.9830\n",
      " 110000/ 200000: 1.9917\n",
      " 120000/ 200000: 2.0408\n",
      " 130000/ 200000: 1.9760\n",
      " 140000/ 200000: 1.9985\n",
      " 150000/ 200000: 2.0207\n",
      " 160000/ 200000: 2.0675\n",
      " 170000/ 200000: 1.8161\n",
      " 180000/ 200000: 1.7003\n",
      " 190000/ 200000: 1.6085\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor(2.2023, grad_fn=<NllLossBackward0>)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# train\n",
    "MAX_EPOCH = 200000\n",
    "batch_size = 32\n",
    "\n",
    "epochs = []\n",
    "losses = []\n",
    "\n",
    "for epoch in range(MAX_EPOCH):\n",
    "    # create minibatch training data for this pass\n",
    "    batch_idx = torch.randint(0, X_train.shape[0], (batch_size,))\n",
    "\n",
    "    # embed inputs\n",
    "    emb = lookup[X_train[batch_idx]]\n",
    "    x = emb.view(-1, num_emb * input_size)  # concat into 1D vector\n",
    "\n",
    "    # forward pass\n",
    "    for layer in layers:\n",
    "        x = layer(x)\n",
    "\n",
    "    # cross entropy loss, which is just softmax(logits) to get probability, then average negative log likelihood of the probability\n",
    "    loss = F.cross_entropy(x, Y_train[batch_idx])\n",
    "\n",
    "    # reset gradients\n",
    "    for p in parameters:\n",
    "        p.grad = None\n",
    "    \n",
    "    loss.backward()  # backprop to calculate gradients\n",
    "\n",
    "    # lr decay\n",
    "    lr = 0.1 if epoch < 0.5 * MAX_EPOCH else 0.01\n",
    "\n",
    "    # update params\n",
    "    for p in parameters:\n",
    "        p.data += -lr * p.grad\n",
    "\n",
    "    # stats\n",
    "    epochs.append(epoch)\n",
    "    losses.append(loss.item())\n",
    "    if epoch % log_epoch == 0:\n",
    "        print(f'{epoch:7d}/{MAX_EPOCH:7d}: {loss.item():.4f}')\n",
    "\n",
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "492015c89e27cc3e5a8c578383431dd8443ebbc21376cec0740bf401bea83bd7"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
